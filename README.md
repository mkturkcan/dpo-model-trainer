# dpo-model-trainer
Train LLMs with DPO with a straightforward notebook using Gemma2.
